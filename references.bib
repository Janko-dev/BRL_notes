% BibTex
@inproceedings{Zhang2015DeepDiveAD,
  title={DeepDive: A Data Management System for Automatic Knowledge Base Construction},
  author={Ce Zhang},
  year={2015}
}

@book{barber,
  added-at = {2011-10-18T22:06:35.000+0200},
  author = {Barber, D.},
  biburl = {https://www.bibsonomy.org/bibtex/2747e0caaf2401d56e4b4759b24abb84d/jil},
  edition = {04-2011},
  interhash = {ad2654b2f528b6b57894570a9ab8069a},
  intrahash = {747e0caaf2401d56e4b4759b24abb84d},
  keywords = {bayes bayesian book free learning machine ml online},
  note = {In press},
  publisher = {{Cambridge University Press}},
  timestamp = {2013-11-23T20:11:51.000+0100},
  title = {{Bayesian Reasoning and Machine Learning}},
  url = {http://www.cs.ucl.ac.uk/staff/d.barber/brml},
  year = 2011
}

@BOOK {workbook,
    author    = "A.J. Hommersom and J. Lodder",
    title     = "Bayesian Reasoning and Learning",
    publisher = "Open Universiteit",
    year      = "2023",
    edition   = "second"
}

@InCollection{paradox-simpson,
	author       =	{Sprenger, Jan and Weinberger, Naftali},
	title        =	{{Simpsonâ€™s Paradox}},
	booktitle    =	{The {Stanford} Encyclopedia of Philosophy},
	editor       =	{Edward N. Zalta},
	howpublished =	{\url{https://plato.stanford.edu/archives/sum2021/entries/paradox-simpson/}},
	year         =	{2021},
	edition      =	{{S}ummer 2021},
	publisher    =	{Metaphysics Research Lab, Stanford University}
}

@ARTICLE {spn,
author = {R. Sanchez-Cauce and I. Paris and F. Diez},
journal = {IEEE Transactions on Pattern Analysis \& Machine Intelligence},
title = {Sum-Product Networks: A Survey},
year = {2022},
volume = {44},
number = {07},
issn = {1939-3539},
pages = {3821-3839},
abstract = {A sum-product network (SPN) is a probabilistic model, based on a rooted acyclic directed graph, in which terminal nodes represent probability distributions and non-terminal nodes represent convex sums (weighted averages) and products of probability distributions. They are closely related to probabilistic graphical models, in particular to Bayesian networks with multiple context-specific independencies. Their main advantage is the possibility of building tractable models from data, i.e., models that can perform several inference tasks in time proportional to the number of edges in the graph. They are somewhat similar to neural networks and can address the same kinds of problems, such as image processing and natural language understanding. This paper offers a survey of SPNs, including their definition, the main algorithms for inference and learning from data, several applications, a brief review of software libraries, and a comparison with related models.},
keywords = {probabilistic logic;artificial neural networks;probability distribution;neural networks;bayes methods;task analysis;inference algorithms},
doi = {10.1109/TPAMI.2021.3061898},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {jul}
}

@misc{poon2012sumproduct,
      title={Sum-Product Networks: A New Deep Architecture}, 
      author={Hoifung Poon and Pedro Domingos},
      year={2012},
      eprint={1202.3732},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@Misc{Pearl2016,
author={Pearl, Judea
and Glymour, Madelyn
and Jewell, Nicholas P.},
title={Causal inference in statistics : a primer},
year={2016},
publisher={John Wiley {\&} Sons Ltd},
address={Chichester, West Sussex, UK},
isbn={9781119186854; 1119186854; 9781119186861; 1119186862; 1119186846; 9781119186847},
language={eng}
}

@book{causal,
author = {Peters, Jonas and Janzing, Dominik and Schlkopf, Bernhard},
title = {Elements of Causal Inference: Foundations and Learning Algorithms},
year = {2017},
isbn = {0262037319},
publisher = {The MIT Press},
abstract = {A concise and self-contained introduction to causal inference, increasingly important in data science and machine learning. The mathematization of causality is a relatively recent development, and has become increasingly important in data science and machine learning. This book offers a self-contained and concise introduction to causal models and how to learn them from data. After explaining the need for causal models and discussing some of the principles underlying causal inference, the book teaches readers how to use causal models: how to compute intervention distributions, how to infer causal models from observational and interventional data, and how causal ideas could be exploited for classical machine learning problems. All of these topics are discussed first in terms of two variables and then in the more general multivariate case. The bivariate case turns out to be a particularly hard problem for causal learning because there are no conditional independences as used by classical methods for solving multivariate cases. The authors consider analyzing statistical asymmetries between cause and effect to be highly instructive, and they report on their decade of intensive research into this problem. The book is accessible to readers with a background in machine learning or statistics, and can be used in graduate courses or as a reference for researchers. The text includes code snippets that can be copied and pasted, exercises, and an appendix with a summary of the most important technical concepts.}
}